<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式系统之CAP理论(1)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8BCAP%E7%90%86%E8%AE%BA-1%2F</url>
    <content type="text"><![CDATA[CAP理论2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 CAP理论为：一个分布式系统最多只能同时满足一致性(Consistency)、可用性(Availability)和分区容错性(Partition tolerance)这三项中的两项。 Consistency一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致。 对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。 一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。 Availability可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间 对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的可用性的时候，都是通过停机时间来计算的。 Partition tolerance分区容错性指“the system continues to operate despite arbitrary message lossor failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。 简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。 CAP理论中的CA和数据库事务中ACID的CA并完全是同一回事儿. 两者之中的A都是C都是一致性(Consistency).CAP中的A指的是可用性 (Availability),而ACID中的A指的是原子性(Atomicity),切勿混为一谈. CAP权衡CA without P分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。 比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。 CP without A如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况,就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？ 在我的Zookeeper介绍（二）——Zookeeper概述一文中其实介绍过zk关于CAP的思考，这里再简单回顾一下： ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持 同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。 AP without C要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的(但是可能实际已经没票了)，你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式系统之一致性算法Raft(6)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8B%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95Raft-6%2F</url>
    <content type="text"></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式系统之一致性算法Paxos(5)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8B%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95Paxos-5%2F</url>
    <content type="text"></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式系统之3PC(4)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8B3PC-4%2F</url>
    <content type="text"><![CDATA[前言三阶段提交对二阶段提交存在的问题进行了改进： 引入超时机制 - 同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。 3PC定义三阶段提交(Three-phase commit), 是二阶段提交的改进版本。所谓的三个阶段分别是：询问，然后再锁资源，最后真正提交。 第一阶段：CanCommit 第二阶段：PreCommit 第三阶段：Do Commit 3PC过程一、CanCommit协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态；否则反馈No。 二、PreCommit协调者在得到所有参与者的响应之后，会根据结果执行2种操作：执行事务预提交，或者中断事务。 执行事务预提交 发送预提交请求 协调者向所有参与者节点发出 preCommit 的请求，并进入 prepared 状态 事务预提交 参与者受到 preCommit 请求后，会执行事务操作，对应 2PC 准备阶段中的“执行事务”，也会 Undo 和 Redo 信息记录到事务日志中 各参与者响应反馈 如果参与者成功执行了事务，就反馈 ACK 响应，同时等待指令：提交（commit） 或终止（abort）。 中断事务 发送中断请求 协调者向所有参与者节点发出 abort 请求 中断事务 参与者如果收到 abort 请求或者超时了，都会中断事务 三、DoCommit该阶段进行真正的事务提交，分为执行提交，或中断事务。 执行提交 发送提交请求 协调者接收到各参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。 事务提交 参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈 事务提交完之后，向协调者发送 ACK 响应 完成事务 协调者接收到所有参与者的 ACK 响应之后，完成事务 中断事务协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 发送中断请求 协调者向所有参与者发送 abort 请求。 事务回滚 参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果 参与者完成事务回滚之后，向协调者发送 ACK 消息。 中断事务 协调者接收到参与者反馈的 ACK 消息之后，完成事务的中断。 优缺点 优点相对于二阶段提交，三阶段提交主要解决的单点故障问题，并减少了阻塞的时间。因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。 缺点三阶段提交也会导致数据一致性问题。由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式系统之2PC(3)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8B2PC-3%2F</url>
    <content type="text"><![CDATA[前言由于BASE理论需要在一致性和可用性方面做出权衡，因此涌现了很多关于一致性的算法和协议。其中比较著名的有二阶提交协议（2 Phase Commitment Protocol），三阶提交协议（3 PhaseCommitment Protocol）和Paxos算法。 本文要介绍的2PC协议，分为两个阶段提交一个事务。并通过协调者和各个参与者的配合，实现分布式一致性。 角色 XA概念 作用 协调者 事务管理器 协调各个参与者,对分布式事务进行提交或回滚 参与者 资源管理器 分布式集群中的节点 分布式事务分布式事务是指会涉及到操作多个数据库的事务。目的是为了保证分布式系统中的数据一致性关键： 需要记录事务在任何节点所做的所有动作 事务进行的所有操作要么全部提交，要么全部回滚 XA规范XA规范是由 X/Open组织（即现在的 Open Group ）定义的分布式事务处理模型。 X/Open DTP模型（ 1994 ）包括： 应用程序（ AP ） 事务管理器（ TM ）：交易中间件等 资源管理器（ RM ）：关系型数据库等 通信资源管理器（ CRM ）：消息中间件等 XA规范定义了交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。而XA接口函数由数据库厂商提供 二阶提交协议和三阶提交协议就是基于XA规范提出的其中，二阶段提交就是实现XA分布式事务的关键。 XA规范的流程，大致如图所示： 2PC定义每个参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报，决定各参与者是否要提交操作还是中止操作。 准备阶段准备阶段分为三个步骤： 事务询问协调者向所有的参与者询问，是否准备好了执行事务，并开始等待各参与者的响应。 执行事务各参与者节点执行事务操作。如果本地事务成功，将Undo和Redo信息记入事务日志中，但不提交；否则，直接返回失败，退出执行。 各参与者向协调者反馈事务询问响应如果参与者成功执行了事务操作，那么就反馈给协调者 Yes响应，表示事务可以执行提交；如果参与者没有成功执行事务，就返回No给协调者，表示事务不可以执行提交。 提交阶段根据准备阶段的投票结果执行2种操作:提交事务或中断事务 提交事务过程 发送提交请求协调者向所有参与者发出commit请求。 事务提交参与者收到commit请求后，会正式执行事务提交操作，并在完成提交之后，释放整个事务执行期间占用的事务资源。 反馈事务提交结果参与者在完成事务提交之后，向协调者发送Ack信息。 事务提交确认协调者接收到所有参与者反馈的Ack信息后，完成事务。 中断事务过程 发送回滚请求协调者向所有参与者发出Rollback请求。 反馈事务回滚结果参与者在完成事务回滚之后，想协调者发送Ack信息。 事务回滚参与者接收到Rollback请求后，会利用其在提交阶段种记录的Undo信息，来执行事务回滚操作。在完成回滚之后，释放在整个事务执行期间占用的资源。 事务中断确认协调者接收到所有参与者反馈的Ack信息后，完成事务中断。 优缺点 优点：原理简单，实现方便。 缺点：同步阻塞，单点问题，数据不一致，容错性不好。 同步阻塞所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。(mapreduce里面是顺序执行) 单点问题协调者是个单点, 如果协调者在提交阶段出现问题，那么整个流程将无法运转 数据不一致协调者向所有的参与者发送commit请求之后，发生了局部网络异常，或者是协调者在尚未发送完所有 commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了commit请求。 容错性不好如果在二阶段提交的提交询问阶段中，参与者出现故障，导致协调者始终无法获取到所有参与者的确认信息，这时协调者只能依靠其自身的超时机制，判断是否需要中断事务。显然，这种策略过于保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式系统之BASE理论(2)]]></title>
    <url>%2F2018%2F11%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B9%8BBASE%E7%90%86%E8%AE%BA-2%2F</url>
    <content type="text"><![CDATA[前言BASE理论是由eBay架构师提出的. BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网分布式系统实践的总结, 是基于CAP定律逐步演化而来. 其核心思想是即使无法做到强一致性, 但每个应用都可以根据自身业务特点，才用适当的方式来使系统达到最终一致性 BASE理论简介BASE理论是Basically Available(基本可用), Soft State(软状态), Eventually Consistent(最终一致性)三个短语的缩写 其核心思想是： 既是无法做到强一致性, 但每个应用可以根据自身的业务特点, 采用适当的方式来是系统达到最终一致性 BASE理论的内容基本可用当系统出现了不可预知故障, 但还是能用, 就是基本可用。。。 响应时间的止损失：正常情况下的搜索引擎0.5秒即返回给用户结果，而基本可用的搜索引擎可以在2秒作用返回结果 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。 软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。 最终一致性在一定期限后，应当保证所有副本数据一致性，从而达到数据的最终一致性。时间期限取决于网络延迟、系统负载、数据复制方案设计等 在实际工程实践中，最终一致性分为5种： 因果一致性（Causal consistency）如果节点A在更新了数据后通知了B，那么B对该数据的访问都是基于A更新修改后的值。与此同时,和节点A无因果关系的节点C的数据访问没有这样的限制 读己之所写（Read your writes）节点A更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性 会话一致性（Session consistency）对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现 “读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。 单调读一致性（Monotonic read consistency）如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。 单调写一致性（Monotonic write consistency）一个系统要能够保证来自同一个节点的写操作被顺序的执行。 在实际的实践中，这5种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。 实际上，不只是分布式系统使用最终一致性，关系型数据库在某个功能上，也是使用最终一致性的。比如备份，数据库的复制过程是需要时间的，这个复制过程中，业务读取到的值就是旧的。当然，最终还是达成了数据一致性。这也算是一个最终一致性的经典案例。 小结总体来说BASE理论面向的是大型高可用、可扩展的分布式系统。与传统ACID特性相反，不同于ACID的强一致性模型，BASE提出通过牺牲强一致性来获得可用性，并允许数据段时间内的不一致，但是最终达到一致状态。同时，在实际分布式场景中，不同业务对数据的一致性要求不一样。因此在设计中，ACID和BASE理论往往又会结合使用。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法图解笔记]]></title>
    <url>%2F2018%2F11%2F05%2F%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[数据结构笔记]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一章 绪论冒泡排序 遍历A[0, n] 依次比较相邻两个数, 若A[i-1]小于A[i], 则交换, 并设置循环遍历标记; 一次循环必然可确定一个最大值, n递减; 1234567891011121314151617void bubble_sort(int A[], n)&#123; bool bSort = false; while(bSort) &#123; bSort = false; for(int i=1; i&lt;n; i++) &#123; if(A[i-1]&gt;A[i]) &#123; swap(A[i-1], A[i]); bSort = true; &#125; &#125; n--; &#125;&#125; 复杂度度量大O记号性质a) 对于任意常数c&gt;0, 有$ O(f(n)) = O(c*f(n)) $b) 对于任意常数a&gt;b&gt;0, $ 有O(n^a + n^b) = O(n^a) $冒泡时间复杂度: $ T(n)=O(2(n-1)^2) = O(2n^2 + 4n + 2) = O(2n^2) = O(n^2) $ $ \Omega $ 标记为最乐观的下限复杂度, $ \Theta $ 上限复杂度 复杂度分析常数：$ T(n) = O(3) + O(2) + O(1) = O(7) = O(1) $对数：$ O(\log_2 n) = O(\log n) $指数：$ O(a^n) $ 1234567891011// 统计整数n二进制展开中数位1的个数int countOnes(unsigned int n)&#123; int num = 0; while(n&gt;0) &#123; num += (1&amp;n); n &gt;&gt; 1; &#125; return num;&#125; 递归线性递归二分递归多分支递归ADT]]></content>
  </entry>
  <entry>
    <title><![CDATA[失败是成功他妈]]></title>
    <url>%2F2018%2F11%2F01%2F%E5%A4%B1%E8%B4%A5%E6%98%AF%E6%88%90%E5%8A%9F%E4%BB%96%E5%A6%88%2F</url>
    <content type="text"><![CDATA[思路清奇小土刀 徐刘根的JAVA面试大杂烩Google的面试Java后端知识体系总结系统设计入门]]></content>
  </entry>
  <entry>
    <title><![CDATA[网络编程实践]]></title>
    <url>%2F2018%2F11%2F01%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[select1234567891011121314#include &lt;sys/select.h&gt;int select(int fd, fd_set* read, fd_set* write, fd_set* excp, struct timeval* tm);// 返回值：就绪描述符的数目，超时返回0，出错返回-1FD_ZERO(fd_set* fds);FD_SET(int fd, fd_set* fds);FD_CLR(int fd, fd_set* fds);FD_ISSET(int fd, fd_set* fds);#include &lt;sys/time.h&gt;struct timeval&#123;__time_t tv_sec; /* Seconds. */__suseconds_t tv_usec; /* Microseconds. */&#125;; 123456789101112131415while(1)&#123; fd_set fds; FD_ZERO(&amp;fds); FD_SET(sock, &amp;fds); int ret = select(fd+1, &amp;fds, 0, 0, NULL); /*NULL一直阻塞*/ if(FD_ISSET(sock, &amp;fds)) &#123; //read &#125;&#125;// 套接字的阻塞非阻塞不影响select ， 只会影响read/write. poll123456789101112131415161718#include &lt;poll.h&gt;int poll(struct pollfd fds[], unsigned int nfds, int timeout);param: fds 数组 nfds 描述符个数，无限制 timeout 阻塞时间，单位msreturn: 返回值 &gt;0 实际发生事件描述符总数 , ==0 超时, -1 失败 设置errno struct pollfd&#123; int fd; // 文件描述符 short events; // 等待事件 short revevents; // 实际发生事件&#125;;POLLIN | POLLPRI 读, POLLOUT | POLLWRBAND 写 123456789101112131415161718192021struct pollfd fds[OPEN_MAX];fds[0].events = POLLIN | POLLPRI;for(;;)&#123; switch(poll(&amp;fds, 1, timeout)) &#123; case 0: printf(&quot;timeout \n&quot;); case -1: printf(&quot;poll error \n&quot;); default: &#123; printf(&quot;some events \n&quot;); if(fds[0].revevents &amp; POLLIN) &#123; // accept and put into fds &#125; &#125; break; &#125;&#125; epoll1234567891011121314151617#include &lt;sys/epoll.h&gt;int epoll_create(int size); // 监听数量int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event);int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);struct epoll_event&#123; __uint32_t events; epoll_data_t data;&#125;EPOLLIN/EPOLLOUT/EPOLLPRI/EPOLLERR/EPOLLHUP/EPOLLLET/EPOLLONESHOT工作模式： LT:应用程序可以不处理，下次还会再响应。 ET:需要立即处理，下次不会响应，默认ET，只响应一次。 必须使用非阻塞套接口，避免饿死其他套接口。 1234567891011121314151617181920212223242526272829void do_epool()&#123; int epollfd; struct epoll_event events[10]; epollfd = epoll_create(1024); events[0].events = EPOLLIN; events[0].data.fd = fd; epoll_ctl(epollfd, EPOLL_CTL_ADD, fd, &amp;(events[0])); for(;;) &#123; struct epoll_event eventArr[100]; // epoll_wait成功之后，储存所有的读写事件 num = epoll_wait(epollfd, eventArr, 1024, -1); for(int = 0; i&lt;num; ++i) &#123; if(eventArr[i].data.fd == lintenfd &amp;&amp;eventArr[i].events &amp; EPOLLIN) // accept else if(eventArr[i].events &amp; EPOLLIN) // 可读，有数据到来 // read else if(eventArr[i].events &amp; EPOLLOUT) // 可写，缓冲区从满==&gt;未满 // write else if(&amp; EPOLLHUP) //RST响应,在epoll上会响应为EPOLLHUP // do something &#125; &#125; close(epollfd); // &#125; 总结select的几大缺点： 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大select支持的文件描述符数量太小了，默认是1024 差异select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。 这也能节省不少的开销。阻塞/非阻塞设置12345#include &lt;fcntl.h&gt;fcntl(FD, F_SETFL, O_NONBLOCK);iFlag = 0;if( ioctl( nSock, FIONBIO, &amp;iFlag) &lt; 0) read123456789101112131415161718192021222324252627int readN(char* pBuf, int nByte)&#123; int nLeft = nByte; int nRead = 0; while(nLeft &gt; 0) &#123; nRead = read( m_nSock, pBuf, nLeft); if(nRead &lt; 0) // &#123; if(errno == EINTR) // 收到信号并从信号处理函数返回时，慢系统调用会返回并设 continue; if(errno == EAGAIN) // 示当前暂时没有数据可读，应该稍后读取 continue; &#125; else if(nRead == 0) //接收到对端发送的FIN，表示对端的写端关闭。 &#123; break; &#125; else //读取数据的长度 &#123; nLeft -= nRead; pBuf += nRead; &#125; &#125; reutrn nByte - nLeft;&#125; write12345678910111213141516171819202122232425int writeN(char* pBuf, int nByte)&#123; int nLeft = nByte; int nWrite = 0; while(nLeft &gt; 0) &#123; nWrite = write(m_nSock, pBuf, nLeft); if(nWrite &lt;=0) &#123; if(errno == EINTR) continue; if(errno == EAGAIN) // 水平模式下,如果返回EAGAIN，把socket加入epoll， // 在epoll的驱动下写数据，全部数据发送完毕后，再移出epoll //do something return nWrite; &#125; nLeft -= nWrite; pBuf += nWrite;s &#125; reutrn nByte - nLeft;&#125;//如果向已经关闭的对端调用write, 系统会向程序发送SIGPIPE信号 ET模式下，EPOLLOUT触发条件有：1.缓冲区满–&gt;缓冲区非满；2.同时监听EPOLLOUT和EPOLLIN事件 时，当有IN 事件发生，都会顺带一个OUT事件； 3.一个客户端connect过来，accept成功后会触发一次OUT事件。踩过的坑errnoerrno是线程安全的, 在一个线程中设置它, 不会影响别的线程对它的使用如果你的程序对它有依赖, 需要开发人员在接口错误处理中手工设置 粘包客户端没有收完整, 导致收下一个包core]]></content>
  </entry>
  <entry>
    <title><![CDATA[网络模型概念]]></title>
    <url>%2F2018%2F11%2F01%2F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[在Linux中，对于一次读取IO的操作, 包含两个阶段 1 Waiting for the data to be ready(等待数据到达内核缓冲区)2 Copying the data from the kernel to the process(从内核缓冲区拷贝数据到程序缓冲区) 对于同步、异步IO, Stevens给的定义 A synchronous I/O operation causes the requesting process to beblocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 根据IO操作和进程的关系，分为五种模型 阻塞IOIO操作的两个阶段都阻塞, 用户进程一直等待系统调用返回 非阻塞IO前三次调用立即返回, 第四次调用内核数据已经准备好, 但是从内核缓冲区拷贝数据到程序缓冲区时用户进程会等待系统调用返回 IO复用select等待数据到达内核缓冲区(或超时), recvfrom从内核缓冲区拷贝数据到程序缓冲区, 两个过程用户进程分别会阻塞等待调用返回 信号驱动设置socket为一个信号驱动IO, 内核数据准备好后通知用户进程.用户进程调用recform, 等待从内核缓冲区拷贝数据到程序缓冲区, 这个过程用户进程阻塞等待 异步IO用户进程调用aio_read后, 可以继续执行, 等待IO操作两个阶段完成收到信号通知, 读取数据 总结前四种都是同步型IO操作, 只有异步IO才是异步型IO操作。]]></content>
  </entry>
  <entry>
    <title><![CDATA[一致性hash算法]]></title>
    <url>%2F2018%2F10%2F30%2F%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常规思路： 1.是什么2.解决了什么问题3.是怎么实现的 hash问题数据库中分库分表规则，按照hash取值、取模、按类别、按某一个字段。例如redis集群使用hash的方式，对图片缓存, 对服务器的数量进行取模hash(a.png)%4 = 2 当我们增加或减少一台服务器时，hash(a.png)%5=? hash(a.png)%3=？redis缓存的图片就找不到了，都会想后端数据库直请求，引发缓存雪崩 一致性hash一致性Hash算法是对2^32取模, 整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形）整个哈希环如下: 整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1，0和2^32-1在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。 下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下： 下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下： 将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！ 例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下： 根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 一致性Hash算法的容错性和可扩展性一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 Hash环的数据倾斜问题一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，使很少的服务节点也能做到相对均匀的数据分布]]></content>
  </entry>
  <entry>
    <title><![CDATA[准备好吹牛B]]></title>
    <url>%2F2018%2F10%2F29%2F%E5%87%86%E5%A4%87%E5%A5%BD%E5%90%B9%E7%89%9BB%2F</url>
    <content type="text"><![CDATA[BOSS系统介绍 整体功能架构计费功能架构计费技术架构计费对外接口设计部署方案(接入/应用/数据)高可用及容灾设计和测试 遇到过的问题 网络粘包, errno死锁binlog乱序, 索引优化内存泄漏(lua,new/del)性能优化, 主机问题(linux时钟源, ssd调度方式cfq/deadline, ck和binglog文件rename) 最得意的事]]></content>
  </entry>
  <entry>
    <title><![CDATA[TCP协议相关概念]]></title>
    <url>%2F2018%2F10%2F29%2FTCP%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[1. TCP协议头部格式 DNS在应用层, TCP/UDP/PORT在传输层, IP在网络层, ARP在数据链路层 为什么需要2MSL？ Maximum Segment Lifetime 报文最大生存时间, 保证最后发送的ACK报文对端可以收到, 不然对端会重发FIN. 所以TIME_WAIT用来重发可能丢失的ACK 2. 三次握手四次分手 为什么需要三次握手？ 为了防止已经失效的连接请求报文段突然又传到服务端，因而产生错误比如A发送sync给B, 在网络延迟N时间后B才收到, B回复ack, 此时A可能已经关闭,B浪费了这个链接的资源 为什么需要四次分手？ 为了确保数据能够完成传输(确保对端收完数据) 3. TCP状态转换 4. TCP如何保证可靠传输三次握手, seq+ack, 超时重传, 流量控制, 拥塞控制 超时重传流量控制拥塞控制5. TCP粘包TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包, 从接收缓冲区看,后一包数据的头紧接着前一包数据的尾 产生原因 发送方 TCP默认会使用Nagle算法:1 只有上一个分组得到确认，才会发送下一个分组2 收集多个小分组，在一个确认到来时一起发送接受方 没有立即处理, TCP将收到的分组保存至接收缓存里, 缓冲区会存在多个包 解决办法 发送发关闭Nagle算法, TCP_NODELAY选项接受方 TCP协议没有处理机制, 通过应用层来处理应用层 定义消息包头(len+type)和包体(data), 收包时循环处理 6. TCP的四种定时器 重传计时器：Retransmission Timer坚持计时器：Persistent Timer保活计时器：Keeplive Timer时间等待计时器：Timer_Wait Timer]]></content>
  </entry>
  <entry>
    <title><![CDATA[c++类型转换]]></title>
    <url>%2F2018%2F10%2F28%2Fc-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[四种类型转换 const_cast 修改类型的const或volatile属性 static_cast 通常用于转换数值类型, 进行非多态的类型转换, 编译时检查 dynamic_cast 基类转换成子类, 基类必须要有虚函数 reinterpret_cast 比较底层的转换, 在非相关的类型之间转换; 操作结果只是简单的从一个指针到别的指针的值的二进制拷贝;在类型之间指向的内容不做任何类型的检查和转换 语法 xxx_cast (expression)]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++智能指针]]></title>
    <url>%2F2018%2F10%2F28%2Fc-%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[智能指针 (头文件memory) auto_ptr c++11已摒弃, 所有权控制不够严格, 没有避免潜在的内存崩溃问题 shared_ptr 采用引用计数, 将一个原始指针分配给多个所有者; 大小为两个指针,一个用于对象，另一个用于包含引用计数的共享控制块(strong ref/weak ref) weak_ptr 提供对一个或多个shared_ptr实例拥有的对象的访问, 但不参与引用计数; 用于观察某个对象但不需要其保持活动状态 unique_ptr 只允许基础指针的一个所有者, 可移动, 但不可复制; 大小等同于一个指针且支持 rvalue 引用 auto_ptr废弃原因： 当把一个auto_ptr赋给另外一个auto_ptr时, 原指针变为野指针, 不安全 1234567891011void Fun(auto_ptr&lt;Test&gt; p1 )&#123; cout&lt;&lt;p1-&gt;m_a&lt;&lt;endl;&#125;void main( )&#123; std::auto_ptr&lt;Test&gt; p( new Test(5) ); Fun(p); cout&lt;&lt;p-&gt;m_a&lt;&lt;endl;&#125; auto_ptr不能指向一组对象, 不能和操作符new[]一起用, 会产生一个运行时错误 1234void main( )&#123; std::auto_ptr&lt;Test&gt; p(new Test[5]);&#125; auto_ptr不能和标准容器（vector,list,map….)一起使用 shared_ptr使用方式 为什么要尽量使用 make_shared为了节省一次内存分配, 原来 shared_ptr x(new Foo); 需要为 Foo 和 ref_count各分配一次内存, 现在用 make_shared()的话, 可以一次分配一块足够大的内存, 供 Foo和 ref_count 对象容身, 数据结构是： explicit构造函数, 不允许隐式转换 123456shared_ptr&lt;double&gt; pd; double *p_reg = new double;pd = p_reg; // not allowed (implicit conversion)pd = shared_ptr&lt;double&gt;(p_reg); // allowed (explicit conversion)shared_ptr&lt;double&gt; pshared = p_reg; // not allowed (implicit conversion)shared_ptr&lt;double&gt; pshared(p_reg); // allowed (explicit conversion) 方法 get(): 获取shared_ptr绑定的资源. reset(): 释放关联内存块的所有权，如果是最后一个指向该资源的shared_ptr,就释放这块内存 unique: 判断是否是唯一指向当前内存的shared_ptr operator bool : 判断当前的shared_ptr是否指向一个内存块，可以用if 表达式判断 存在问题 多个shared_ptrs对象用一个普通指针构造, 析构时core 123456void main( )&#123; int* p = new int; shared_ptr&lt;int&gt; sptr1( p); shared_ptr&lt;int&gt; sptr2( p );&#125; 用指针去创建shared_ptr, 不小心删除指针, 析构时core 循环引用, 资源都不会正常释放 12345678910void main( )&#123; shared_ptr&lt;B&gt; sptrB( new B ); shared_ptr&lt;A&gt; sptrA( new A ); // sptrB-&gt;m_sptrA shared_ptr sptrB-&gt;m_sptrA = sptrA; sptrA-&gt;m_sptrB = sptrB;&#125;4. 读线程不安全, 写线程不安全 weak_ptr使用方式 方法 调用lock()可以得到shared_ptr或者直接将weak_ptr转型为shared_ptr 调用use_count()去获取引用计数，该方法只返回强引用计数，并不返回弱引用计数 调用expired()方法。比调用use_count()方法速度更快 解决循环引用内存不会释放问题123456789void main( )&#123; shared_ptr&lt;B&gt; sptrB( new B ); shared_ptr&lt;A&gt; sptrA( new A ); // sptrB-&gt;m_sptrA weak_ptr sptrB-&gt;m_sptrA = sptrA; sptrA-&gt;m_sptrB = sptrB; sptrA-&gt;PrintSpB( ); &#125; unique_ptr优点 将一个unique_ptr赋值给另一个时, 如果源 unique_ptr是个临时右值, 编译器允许这么做 1234567unique_ptr&lt;string&gt; demo(const char * s)&#123; unique_ptr&lt;string&gt; temp (new string (s))； return temp；&#125;unique_ptr&lt;string&gt; ps;ps = demo(&apos;Uniquely special&quot;)； 无法赋值, 可以移动; 或者release释放后reset转移 12unique_ptr&lt;Foo&gt; ptr = make_unique&lt;Foo&gt;();Foo* p = ptr.release();]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gstack]]></title>
    <url>%2F2018%2F10%2F25%2Fgstack%2F</url>
    <content type="text"><![CDATA[123456#!bin/shwhile( true )do gstack $1&gt;&gt;gstack_$1.txtsleep 1done]]></content>
      <categories>
        <category>gdb</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Posix线程编程指南]]></title>
    <url>%2F2018%2F10%2F24%2FPosix%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[摘自 IBM developerWorks 图书频道 Posix线程编程指南线程创建与取消 线程私有数据线程同步线程终止杂项 通用线程：POSIX 线程详解一种支持内存共享的简捷工具称作互斥对象的小玩意使用条件变量提高效率 线程池的介绍及简单实现c++ 内存池]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux进程间通信]]></title>
    <url>%2F2018%2F10%2F24%2FLinux%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[摘自 IBM developerworks 郑彦兴 深刻理解Linux进程间通信(IPC) Linux环境进程间通信（一）Linux环境进程间通信（二）信号（上）Linux环境进程间通信（二）信号（下）Linux环境进程间通信（三）消息队列Linux环境进程间通信（四）信号灯Linux环境进程间通信（五）共享内存（上）Linux环境进程间通信（五）共享内存（下）Linux 环境进程间通信（六）套接口]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zk原理和实践]]></title>
    <url>%2F2018%2F10%2F24%2Fzk%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2018%2F10%2F24%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
      <categories>
        <category>c++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[性能之巅 笔记]]></title>
    <url>%2F2018%2F10%2F24%2F%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[c++ 编程思想 笔记]]></title>
    <url>%2F2018%2F10%2F24%2Fc-%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC框架原理和实践]]></title>
    <url>%2F2018%2F10%2F24%2FRPC%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[简单的RPC框架有三部分组成： 1 服务提供者，运行在服务端，负责提供服务接口定义和服务实现类2 服务发布者，运行在RPC服务端，负责将本地服务发布成远程服务，供其他消费者调用3 本地服务代理，运行在RPC客户端，通过代理调用远程服务提供者，然后将结果进行封返回给本地消费者 RPC框架的调用原理如图： 服务治理问题在大规模服务化之前，应用可能只是通过RPC框架，简单的暴露和引用远程服务，通过配置URL地址进行远程服务调用，路由则通过F5负载均衡器等进行简单的负载均衡。 当服务越来越多的时候，服务的URL配置管理变得更加困难。单纯的使用RPC就有点吃不消。所以在大规模分布式集群中，RPC只是作为集群的一个方法调用手段。 RPC框架实现的几个核心技术点： 远程服务提供者 需要以某种形式(url/idl)提供给服务调用者 远程代理对象 服务调用者调用的服务实际是远程服务的本地代理 通信： 与具体协议无关 序列化：远程通信需要将对象转成二进制进行传输，不同序列化框架，支持的数据类型数据包大小，及性能差异很大 PRC框架高性能设计 I/O调度模型：同步阻塞(BIO) 还是非阻塞(NIO) 序列化框架的选择：文本协议、二进制协议、压缩后的二进制协议 线程调度模型： 串行或是并行调度，锁竞争还是无锁化 实践消息定义Header: TaskCreateTime/TaskTimeOut/SessionId/TenantldBody: Session String/SDL stream/ErrorInfo 序列化根据不同的序列化框架做比对测试 框架版本 测试内容 处理次数 耗时(us) 每秒处理次数 每次耗时(us) 1.8.x SJSON序列化 10000 416,225 24,025 41.62 1.8.x SJSON反序列化 10000 581,872 17,185 58.19 2.1.0 SCDR序列化 500000 2,716,170 184,083 5.43 2.1.0 SCDR反序列化 500000 3,409,015 146,670 6.82 2.1.0 SJSON序列化 10000 1,145,281 8,731 115 2.1.0 SJSON反序列化 10000 954,927 10,472 95.5 2.2.0 SJSON序列化 10000 427225 23,407 42.72 2.2.0 SJSON反序列化 10000 505211 19,794 50.52 RPC服务端模型设计1 纯粹单线程模型 所有工作在一个线程里实现采用非阻塞I/O实现单线程处理能力最大化，但没有扩展能力适用于连接数少、负载轻的服务场景 2 独立事件轮询线程 + 工作线程组模型 独立的事件轮询线程工作线程可水平扩展（性能非线性扩展）线程间频繁数据交换需要同步机制影响性能工作线程一次只处理一个连接的任务，对于慢速连接效率不高（可能会被阻塞）适用于任务量不大但业务处理耗时较大、网速快且稳定等场景 3 独立端口监听 + I/O及工作线程组模型 独立端口监听线程，获取的新连接转发给指定的工作线程工作线程负责socket连接的I/O时间轮询，以及后续的消息I/O和业务处理采用非阻塞I/O，处理能力随工作线程的扩展而线性扩展适用于连接数大、任务量大但业务处理耗时较小的场景 4 独立端口监听 + I/O线程 + BIZ线程分组模型 I/O事件轮询和消息收发由独立I/O线程执行业务处理由独立线程执行并可扩展一个I/O线程和若干和BIZ线程组成一个线程组，并可按组横向扩展适用于连接数多，任务量大，任务处理耗时大的场景]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux 常用命令]]></title>
    <url>%2F2018%2F10%2F24%2Flinux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[系统级计数器vmstat虚拟内存和物理内存的统计 iostat每个磁盘 I/O 的使用情况，由块设备接口报告 mpstat每个CPU的使用情况 freesar 进程级计数器top按一个统计数据排序，显示排名高的进程 ps进程状态，显示进程的各种统计信息，包括内存和CPU的使用 pmap将进程的内存和使用统计一起列出 iotop进程的I/O速度 pidstat查看进程I/O , 内存, CPU 网络工具netstatssiftopnethogstpcdump 其他命令tarfinddugrepddlsof]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker实践]]></title>
    <url>%2F2018%2F10%2F24%2FDocker%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式服务框架原理和实践]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库事务及锁]]></title>
    <url>%2F2018%2F10%2F23%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E5%8F%8A%E9%94%81%2F</url>
    <content type="text"><![CDATA[事务的四个特性 原子性(Atomicity) 一致性(Consistency) 隔离性(Isolation) 持久性(Durability) 原子性一个事务中的全部操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会回滚到事务开始前的状态，不会对数据库有任何影响。 一致性在事务开始之前和事务结束以后，数据库的完整性没有被破坏。拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 隔离性数据库允许多个并发事务对数据进行读写和修改，防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交(Read uncommitted)、读提交(read committed)、可重复读(repeatable read)和串行化（Serializable）。即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务隔离级别如果不考虑事务的隔离性，会发生的几种问题: 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。(读未提交) 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 虚读(幻读)幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户, 如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。幻读和不可重复读都是读取了另一条已经提交的事务(这点就脏读不同)，所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体(比如数据的个数)。 串行化(Serializable)在串行化隔离模式下，消除了脏读，幻象，但事务并发度急剧下降，事务的隔离级别与事务的并发度成反比，隔离级别越高，事务的并发度越低。实际生产环境下，dba会在并发和满足业务需求之间作权衡，选择合适的隔离级别 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 在MySQL数据库中，支持上面四种隔离级别，默认的为Repeatable read (可重复读)；而在Oracle数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为Read committed级别 事务隔离的实现——锁 共享锁(S锁) [] 用于只读操作(SELECT)，锁定共享的资源。共享锁不会阻止其他用户读，但是阻止其他的用户写和修改。 更新锁(U锁) [] 用于可更新的资源中。防止当多个会话在读取、锁定以及随后可能进行的资源更新时发生常见形式的死锁。 独占锁(X锁，也叫排他锁) [] 一次只能有一个独占锁用在一个资源上，并且阻止其他所有的锁包括共享缩。写是独占锁，可以有效的防止“脏读”。 Read Uncommited 如果一个事务已经开始写数据，则另外一个数据则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。 Read Committed 读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。可以通过“瞬间共享读锁”和“排他写锁”实现。 Repeatable Read 读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。可以通过“共享读锁”和“排他写锁”实现。 Serializable 读加共享锁，写加排他锁，读写互斥。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>ACID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux知识体系]]></title>
    <url>%2F2018%2F10%2F19%2Flinux%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[c++知识体系]]></title>
    <url>%2F2018%2F10%2F19%2Fc-%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>c++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gdb多线程调试]]></title>
    <url>%2F2018%2F10%2F15%2Fgdb%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%B0%83%E8%AF%95-1%2F</url>
    <content type="text"><![CDATA[all-stop mode: 默认模式, 有一个线程在断点处停止，其他所有线程也会停止 non-stop mode: 某一个线程停止时，其他线程会继续运行 Background Execution：异步运行程序 Thread-Specific Breakpoints: 控制断点 Interrupted System Calls: gdb会干扰系统调用 Observer Mode：gdb不影响程序执行 all-stop mode当进程在gdb下停止时，所有的线程都停止运行。当用单步调试命令“step或next”，所有的线程开始执行。由于执行线程调度的是操作系统不是gdb，单步调试命令不能让所有的线程都单步。当前线程执行了一步，其他线程可能执行了N步。当执行next/step/continue时，当前线程完成单步运行前，其他线程运行遇到断点/信号/异常，gdb会选择一个遇到短信或信号中断的线程，切换线程时会提示“[Switching to Thread n]” set scheduler-locking设置调度锁定模式，在一些系统中，gdb可以通过锁定操作系统线程调度，只允许一个线程运行。如果是on,单步调试命令会阻止其他线程抢占, 其他线程不会运行。如果是off，所有线程线程都会运行。当执行continue/util/finish 时，其他进程会恢复运行. show scheduler-locking显示当前线程调度锁定状态 set schedule-multiple当执行continue/next/step时，gdb只允许当前进程下的线程恢复运行(fork出过个进程)。on: 所有进程下的线程恢复运行off: 当前进程下的线程恢复运行 show schedule-multiple显示多进程恢复模式 non-stop mode在一些多线程的应用中，gdb支持只停止需要调试的线程，其他线程可运行不受影响。例如某些线程具有实时约束或必须继续响应外部事件，这是最小化的实时调试。称为不间断模式。在non-stop mode中，当一个线程因为断点停止时，其他线程正常运行，continue/step 只适用于当前线程。一般情况下在gdb启动或attach 一个进程时设置non-stop mode, 顺序执行如下命令，进入non-stop mode: Enable the async interface. set target-async 1 If using the CLI, pagination breaks non-stop. set pagination off Finally, turn it on! set non-stop on continue -a, 让所有线程都继续执行, continue 只能让当前线程继续执行interrupt -a, 停止整个程序, interrupt/Ctrl-c 只能让当前线程挂起, 其他命令不支持-a. Background Execution基本上用不到 Thread-Specific Breakpointsbreak linespec thread threadnobreak linespec thread threadno if ..threadno 从 info threads 中得到.比如(gdb) break frik.c:13 thread 28 if bartab &gt; lim [ ] Interrupted System Calls在使用gdb调试多线程程序时，有一个副作用。如果一个线程因断点或其他原因而停止，而另一个线程在系统调用中被阻塞，那么系统调用可能会提前返回。这是多线程和gdb用来实现断点和其他停止执行的事件的信号之间交互的结果。例如： sleep (10); 如果不同的线程在断点处或出于其他原因停止，则调用sleep将提前返回。 int unslept = 10; while (unslept &gt; 0) unslept = sleep (unslept); 允许系统调用提前返回，因此系统仍然符合其规范。但是gdb确实会导致多线程程序的行为与没有gdb时不同。另外，gdb在线程库中使用内部断点来监视某些事件，例如线程创建和线程销毁。当这样的事件发生时，另一个线程中的系统调用可能会提前返回，即使您的程序似乎没有停止. Observer Mode略]]></content>
      <categories>
        <category>gdb</category>
      </categories>
      <tags>
        <tag>gdb</tag>
      </tags>
  </entry>
</search>
